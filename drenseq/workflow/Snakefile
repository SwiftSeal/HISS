import pandas as pd
from Bio import SeqIO
from snakemake.utils import validate

configfile: "config/config.yaml"

samples = pd.read_table(config["samples"], header = 0).set_index(["sample"], drop = False)
validate(samples, "samples_schema.yaml")

if samples.duplicated(subset = ["sample"]).any():
    bad_samples = print(samples[samples.duplicated(subset = ["sample"])]["sample"].tolist())
    sys.exit(f"Duplicate sample in samples file, check your inputs! Bad samples are: {bad_samples}")

bed_dict = {}
with open(config["CDS_Bed"]) as bed:
    for line in bed.readlines():
        line = line.rstrip()
        line_split = line.split()
        bed_dict[line_split[0]] = int(line_split[2])

for gene in SeqIO.parse(config["Reference_Fasta"], "fasta"):
    if bed_dict[gene.id] > len(gene.seq):
        sys.exit("Bed file co-ordinates are out of range of your fasta file, check your inputs!")

def get_F_Reads(wildcards):
    return samples["FRead"][wildcards.sample]


def get_R_Reads(wildcards):
    return samples["RRead"][wildcards.sample]


def get_samples(wildcards):
    return samples["sample"][wildcards.sample]


rule all:
    input:
        "coverage/all_coverage_values_transposed.txt"


rule extract_reference_headers:
    input:
        bed = config["CDS_Bed"],
        fasta = config["Reference_Fasta"]
    output:
        nlrs = "resources/reference_headers_nlrs.txt",
        contigs = "resources/reference_headers_contigs.txt"
    resources:
        mem_mb = 1000
    log:
        "logs/extract_reference_headers/extract_reference_headers.log"
    shell:
        """
        echo "gene" 1> {output.nlrs} 2> {log}
        cat {input.bed} | cut -f4 1>> {output.nlrs} 2>> {log}
        cat {input.fasta} | grep '>' | sed 's/>//g' 1> {output.contigs} 2>> {log}
        """


rule trim_read_remove_adaptor:
    input:
        fastqF = get_F_Reads,
        fastqR = get_R_Reads,
        adaptor_1 = config["adaptor_path_1"],
        adaptor_2 = config["adaptor_path_2"]
    output:
        trimF = temp("trimmed_reads/{sample}/R1.fq"),
        trimR = temp("trimmed_reads/{sample}/R2.fq")
    threads:
        4
    resources:
        mem_mb = 2000
    conda:
        "envs/cutadapt.yaml"
    shell:
        """
        cutadapt --cores {threads} --minimum-length 50 -q 20,20 -a file:{input.adaptor_1} -A file:{input.adaptor_2} -o {output.trimF} -p {output.trimR} {input.fastqF} {input.fastqR}
        """


rule bowtie_build:
    input:
        ref = config["Reference_Fasta"]
    output:
        index = multiext(config["Reference_Fasta"], ".1.bt2", ".2.bt2", ".3.bt2", ".4.bt2", ".rev.1.bt2", ".rev.2.bt2")
    conda:
        "envs/bowtie2_samtools.yaml"
    resources:
        mem_mb = 1000
    shell:
        """
        bowtie2-build {input} {input}
        """


rule bowtie_align:
    input:
        ref = config["Reference_Fasta"],
        FRead = "trimmed_reads/{sample}/R1.fq",
        RRead = "trimmed_reads/{sample}/R2.fq",
        index = config["Reference_Fasta"] + ".1.bt2"
    params:
        rg_id = "{sample}",
        rg = "SM:{sample}",
        score = config["scoreMinRelaxed"],
        max_align = config["maximum_alignments"]
    threads:
        8
    resources:
        mem_mb = 1000
    conda:
        "envs/bowtie2_samtools.yaml"
    output:
        temp("tmp_mappings/{sample}.bam")
    shell:
        """
        bowtie2 -x {input.ref} -1 {input.FRead} -2 {input.RRead} --rg-id {params.rg_id} --rg {params.rg} -p {threads} --score-min {params.score} --phred33 --fr --maxins 1000 --very-sensitive --no-unal --no-discordant -k {params.max_align} | samtools view --threads {threads} -S -b > {output}
        """


rule samtools_sort:
    input:
        "tmp_mappings/{sample}.bam"
    threads:
        4
    resources:
        mem_mb = 1000
    output:
        temp("mappings/{sample}_sorted.bam")
    conda:
        "envs/bowtie2_samtools.yaml"
    shell:
        """
        samtools sort --threads {threads} -l 9 {input} -o {output}
        """


rule samtools_index_relaxed:
    input:
        "mappings/{sample}_sorted.bam"
    output:
        temp("mappings/{sample}_sorted.bam.bai")
    conda:
        "envs/bowtie2_samtools.yaml"
    resources:
        mem_mb = 1000
    shell:
        """
        samtools index {input} {output}
        """


rule sambamba_filter:
    input:
        bam = "mappings/{sample}_sorted.bam",
        bai = "mappings/{sample}_sorted.bam.bai"
    output:
        temp("mappings/{sample}_strict.bam")
    conda:
        "envs/sambamba.yaml"
    resources:
        mem_mb = 1000
    shell:
        """
        sambamba view --format=bam -l 9 --filter='[NM] == 0' -o {output} {input.bam}
        """


rule samtools_index_strict:
    input:
        "mappings/{sample}_strict.bam"
    output:
        temp("mappings/{sample}_strict.bam.bai")
    conda:
        "envs/bowtie2_samtools.yaml"
    resources:
        mem_mb = 1000
    shell:
        """
        samtools index {input} {output}
        """


rule baits_mapping_db:
    input:
        ref = config["Reference_Fasta"]
    output:
        temp(multiext("baits_mapping/baits_mapping_db", ".ndb", ".nhr", ".nin", ".njs", ".not", ".nsq", ".ntf", ".nto"))
    conda:
        "envs/blast.yaml"
    resources:
        mem_mb = 1000
    shell:
        """
        makeblastdb -in {input.ref} -out "baits_mapping/baits_mapping_db" -dbtype nucl
        """


rule baits_mapping:
    input:
        baits = config["baits_sequences"],
        db = multiext("baits_mapping/baits_mapping_db", ".ndb", ".nhr", ".nin", ".njs", ".not", ".nsq", ".ntf", ".nto")
    output:
        blast_result = "baits_mapping/baits_blast.txt"
    params:
        identity = config["BLAST_identity"],
        coverage = config["BLAST_coverage"]
    conda:
        "envs/blast.yaml"
    resources:
        mem_mb = 1000
    shell:
        """
        blastn -db "baits_mapping/baits_mapping_db" -query {input.baits} -out {output} -perc_identity {params.identity} -qcov_hsp_perc {params.coverage} -evalue 1e-5 -outfmt '6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore qlen len qcovs qcovhsp'
        """


rule baits_region:
    input:
        blast = "baits_mapping/baits_blast.txt",
        headers = "resources/reference_headers_contigs.txt",
        fasta = config["Reference_Fasta"]
    params:
        flank = config["Flanking_region"]
    output:
        "baits_mapping/baits_regions.bed"
    conda:
        "envs/bait_mapping.yaml"
    resources:
        mem_mb = 1000
    shell:
        """
        Rscript workflow/script/RangeReduction.R {input.blast} {output} {params.flank} {input.headers} {input.fasta}
        """


rule nlr_annotator_baits:
    input:
        baits_region = "baits_mapping/baits_regions.bed",
        nlr_annotator_region = config["CDS_Bed"]
    output:
        "baits_mapping/nlr_baits_regions.bed"
    conda:
        "envs/bedtools.yaml"
    resources:
        mem_mb = 1000
    shell:
        """
        bedtools intersect -a {input.nlr_annotator_region} -b {input.baits_region} > {output}
        """


rule bait_blast_check:
    input:
        bed = "baits_mapping/nlr_baits_regions.bed",
        headers = "resources/reference_headers_nlrs.txt"
    output:
        passed_genes = "baits_mapping/passed_genes.txt"
    resources:
        mem_mb = 1000
    log:
        missed_genes = "baits_mapping/missed_genes.txt"
    run:
        bed_nlr = set()

        with open(input.bed) as f:
            for line in f:
                bed_nlr.add(line.strip().split()[3])

        with open(input.headers) as f:
            next(f) # skip the header
            for line in f:
                nlr = line.strip()
                if nlr not in bed_nlr:
                    with open(log.missed_genes, "w") as l:
                        string_to_write = nlr + " not found in bed file"
                        print(string_to_write, file = l)
                        raise Exception("Baits not found in NLRs, check the missed genes file")

        with open(output.passed_genes, "w") as f:
            for nlr in bed_nlr:
                print(nlr, file = f)

rule coverage_strict:
    input:
        bam = "mappings/{sample}_strict.bam",
        bed = "baits_mapping/nlr_baits_regions.bed",
        bai = "mappings/{sample}_strict.bam.bai",
        missing_genes = "baits_mapping/passed_genes.txt"
    output:
        "coverage/{sample}_coverage.txt"
    conda:
        "envs/bedtools.yaml"
    resources:
        mem_mb = 1000
    shell:
        """
        coverageBed -d -a {input.bed} -b {input.bam} > {output}
        """


rule per_gene_coverage:
    input:
        referenceGenes = "resources/reference_headers_nlrs.txt",
        sample_coverage = "coverage/{sample}_coverage.txt"
    output:
        gene_coverage = "coverage/{sample}_geneCoverage.txt"
    resources:
        mem_mb = 1000
    shell:
        """
        cat {input.referenceGenes} | tail -n +2 | while read gene; do numPosWithCoverage=`grep -w "$gene" {input.sample_coverage} | awk '$6>0' | wc -l`; numPosTotal=`grep -w "$gene" {input.sample_coverage} | wc -l`; if [ $numPosTotal -eq 0 ]; then echo "ERROR: gene $gene has CDS region of length zero. Check your input data (e.g. gene spelling in FASTA and CDS BED file) and retry.\nAborting pipeline run."; exit; fi; pctCov=`awk "BEGIN {{print ($numPosWithCoverage/$numPosTotal)*100 }}"`; echo -e "\n# covered positions for sample {wildcards.sample} in gene $gene: $numPosWithCoverage\n# CDS positions for gene $gene: $numPosTotal\npctCov: $pctCov"; echo -e "$gene\t$pctCov" >> {output.gene_coverage}; done
        """


rule combine_gene_coverage:
    input:
        "coverage/{sample}_geneCoverage.txt"
    output:
        "coverage/{sample}_coverageValues.txt"
    resources:
        mem_mb = 1000
    shell:
        """
        echo {wildcards.sample} > {output}
        cat {input} | cut -f2 >> {output}
        """


rule combine_coverage_values:
    input:
        gene_names = "resources/reference_headers_nlrs.txt",
        coverage = expand("coverage/{sample}_coverageValues.txt", sample=samples["sample"])
    output:
        "coverage/all_coverage_values.txt"
    params:
        ulimit = config["ulimit"]
    resources:
        mem_mb = 1000
    shell:
        """
        ulimit -n {params.ulimit}
        paste {input.gene_names} {input.coverage} > {output}
        """


rule transpose_combined_coverage:
    input:
        "coverage/all_coverage_values.txt"
    output:
        "coverage/all_coverage_values_transposed.txt"
    resources:
        mem_mb = 1000
    run:
        df = pd.read_table(input[0], header = None)
        df.T.to_csv(output[0], sep = "\t", header = False, index = False)
