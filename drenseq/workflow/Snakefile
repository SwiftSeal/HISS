import pandas as pd
from Bio import SeqIO
from snakemake.utils import validate

configfile: "config/config.yaml"

samples=pd.read_table(config["samples"], header=0).set_index(["sample"], drop=False)
validate(samples, "samples_schema.yaml")

if samples.duplicated(subset=["sample"]).any():
    bad_samples = print(samples[samples.duplicated(subset = ["sample"])]["sample"].tolist())
    sys.exit(f"Duplicate sample in samples file, check your inputs! Bad samples are: {bad_samples}")

bed_dict = {}
with open(config["CDS_Bed"]) as bed:
    for line in bed.readlines():
        line = line.rstrip()
        line_split = line.split()
        bed_dict[line_split[0]] = int(line_split[2])

for gene in SeqIO.parse(config["Reference_Fasta"], "fasta"):
    if bed_dict[gene.id] > len(gene.seq):
        sys.exit("Bed file co-ordinates are out of range of your fasta file, check your inputs!")

def get_F_Reads(wildcards):
    return samples["FRead"][wildcards.sample]


def get_R_Reads(wildcards):
    return samples["RRead"][wildcards.sample]


def get_samples(wildcards):
    return samples["sample"][wildcards.sample]


rule all:
    input:
        "coverage/all_coverage_values_transposed.txt"


rule extract_reference_headers:
    input:
        bed=config["CDS_Bed"],
        fasta=config["Reference_Fasta"]
    output:
        nlrs="resources/reference_headers_nlrs.txt",
        contigs="resources/reference_headers_contigs.txt"
    log:
        "logs/initial_grep/log.txt"
    resources:
        mem_mb=2000
    run:
        shell("""echo "gene" > {output}""")
        shell("""(cat {input.bed} | cut -f4 >> {output.nlrs}) 2> {log}""")
        shell("""(cat {input.fasta} | grep '>' | sed 's/>//g' >> {output.contigs}) 2>> {log}""")


rule trim_read_remove_adaptor:
    input:
        fastqF=get_F_Reads,
        fastqR=get_R_Reads,
        adaptor_1=config["adaptor_path_1"],
        adaptor_2=config["adaptor_path_2"]
    output:
        trimF=temp("trimmed_reads/{sample}/R1.fq"),
        trimR=temp("trimmed_reads/{sample}/R2.fq")
    log:
        "logs/cutadapt/{sample}.log"
    threads:
        8
    resources:
        mem_mb=4000
    conda:
        "envs/cutadapt.yaml"
    shell:
        "cutadapt --cores {threads} --minimum-length 50 -q 20,20 -a "
        "file:{input.adaptor_1} -A file:{input.adaptor_2} -o "
        "{output.trimF} -p {output.trimR} {input.fastqF} "
        "{input.fastqR} > {log}"


rule bowtie_build:
    input:
        ref=config["Reference_Fasta"]
    output:
        index=multiext(config["Reference_Fasta"], ".1.bt2", ".2.bt2", ".3.bt2", ".4.bt2", ".rev.1.bt2", ".rev.2.bt2")
    conda:
        "envs/bowtie2_samtools.yaml"
    log:
        "logs/bowtie2/indexing.log"
    resources:
        mem_mb=2000
    shell:
        "(bowtie2-build {input} {input}) 2> {log}"


rule bowtie_align:
    input:
        ref=config["Reference_Fasta"],
        FRead="trimmed_reads/{sample}/R1.fq",
        RRead="trimmed_reads/{sample}/R2.fq",
        index=config["Reference_Fasta"] + ".1.bt2"
    params:
        rg_id="{sample}",
        rg="SM:{sample}",
        score=config["scoreMinRelaxed"],
        max_align=config["maximum_alignments"]
    threads:
        8
    resources:
        mem_mb=4000
    conda:
        "envs/bowtie2_samtools.yaml"
    output:
        temp("tmp_mappings/{sample}.bam")
    log:
        "logs/bowtie2/{sample}.log"
    shell:
        "(bowtie2 -x {input.ref} -1 {input.FRead} -2 {input.RRead} --rg-id "
        "{params.rg_id} --rg {params.rg} -p {threads} --score-min "
        "{params.score} --phred33 --fr --maxins 1000 --very-sensitive "
        "--no-unal --no-discordant -k {params.max_align} | samtools view --threads {threads} "
        "-S -b > {output}) 2> {log}"


rule samtools_sort:
    input:
        "tmp_mappings/{sample}.bam"
    threads:
        8
    resources:
        mem_mb=8000
    output:
        "mappings/{sample}_sorted.bam"
    conda:
        "envs/bowtie2_samtools.yaml"
    log:
        "logs/samtools_sort/{sample}.log"
    shell:
        "(samtools sort --threads {threads} -l 9 {input} -o {output}) "
        "> {log}"


rule samtools_index_relaxed:
    input:
        "mappings/{sample}_sorted.bam"
    output:
        "mappings/{sample}_sorted.bam.bai"
    conda:
        "envs/bowtie2_samtools.yaml"
    log:
        "logs/samtools_index_relaxed/{sample}.log"
    resources:
        mem_mb=2000
    shell:
        "(samtools index {input} {output}) > {log}"


rule sambamba_filter:
    input:
        bam="mappings/{sample}_sorted.bam",
        bai="mappings/{sample}_sorted.bam.bai"
    output:
        "mappings/{sample}_strict.bam"
    conda:
        "envs/sambamba.yaml"
    log:
        "logs/sambamba/{sample}.log"
    resources:
        mem_mb=2000
    shell:
        """(sambamba view --format=bam -l 9 --filter='[NM] == 0' -o {output} {input.bam}) > {log}"""


rule samtools_index_strict:
    input:
        "mappings/{sample}_strict.bam"
    output:
        "mappings/{sample}_strict.bam.bai"
    conda:
        "envs/bowtie2_samtools.yaml"
    log:
        "logs/samtools_index_strict/{sample}.log"
    resources:
        mem_mb=2000
    shell:
        "(samtools index {input} {output}) > {log}"


rule baits_mapping_db:
    input:
        ref=config["Reference_Fasta"]
    output:
        temp(multiext("baits_mapping/baits_mapping_db", ".ndb", ".nhr", ".nin", ".njs", ".not", ".nsq", ".ntf", ".nto"))
    conda:
        "envs/blast.yaml"
    log:
        "logs/baits_blast_db.log"
    resources:
        mem_mb=2000
    shell:
        """
        (makeblastdb -in {input.ref} -out "baits_mapping/baits_mapping_db" -dbtype nucl) 2> {log}
        """


rule baits_mapping:
    input:
        baits=config["baits_sequences"],
        db=multiext("baits_mapping/baits_mapping_db", ".ndb", ".nhr", ".nin", ".njs", ".not", ".nsq", ".ntf", ".nto")
    output:
        blast_result="baits_mapping/baits_blast.txt"
    params:
        identity=config["BLAST_identity"],
        coverage=config["BLAST_coverage"]
    conda:
        "envs/blast.yaml"
    log:
        "logs/baits_blast.log"
    resources:
        mem_mb=2000
    shell:
        """
        (blastn -db "baits_mapping/baits_mapping_db" -query {input.baits} -out {output} -perc_identity {params.identity} -qcov_hsp_perc {params.coverage} -evalue 1e-5 -outfmt '6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore qlen len qcovs qcovhsp' -num_threads 2) 2> {log}
        """


rule baits_region:
    input:
        blast="baits_mapping/baits_blast.txt",
        headers="resources/reference_headers_contigs.txt",
        fasta=config["Reference_Fasta"]
    params:
        flank=config["Flanking_region"]
    output:
        "baits_mapping/baits_regions.bed"
    conda:
        "envs/bait_mapping.yaml"
    log:
        "logs/baits_regions_catching.log"
    resources:
        mem_mb=2000
    shell:
        "(Rscript workflow/script/RangeReduction.R {input.blast} {output} {params.flank} {input.headers} {input.fasta}) 2> {log}"


rule nlr_annotator_baits:
    input:
        baits_region="baits_mapping/baits_regions.bed",
        nlr_annotator_region=config["CDS_Bed"]
    output:
        "baits_mapping/nlr_baits_regions.bed"
    conda:
        "envs/bedtools.yaml"
    log:
        "logs/nlr_annotator_regions_catching.log"
    resources:
        mem_mb=2000
    shell:
        "(bedtools intersect -a {input.nlr_annotator_region} -b {input.baits_region} > {output}) 2> {log}"


rule bait_blast_check:
    input:
        bed="baits_mapping/nlr_baits_regions.bed",
        headers="resources/reference_headers_nlrs.txt"
    output:
        "baits_mapping/passed_genes.txt"
    log:
        run_log="logs/bait_blast_check.log",
        missed_genes_log="baits_mapping/missing_genes.txt"
    resources:
        mem_mb=2000
    shell:
        """truncate -s 0 {log.missed_genes_log} 2> {log.run_log}
        echo "truncate works"
        cat {input.headers} | tail -n +2 | while read nlr; do blast_hits=$(cat {input.bed} | grep "$nlr" | wc -l); if [ $blast_hits -eq 0 ]; then echo "ERROR: $nlr has no blast hits. Check baits_mapping/missing_genes.txt" >> {log.run_log}; echo "$nlr" >> {log.missed_genes_log}; fi; done
        echo "loop done"
        missing_genes=$(cat {log.missed_genes_log} | wc -l)
        echo "variable set"
        if [ $missing_genes -ne 0 ]; then exit; fi
        echo "exit if done""""


rule coverage_strict:
    input:
        bam="mappings/{sample}_strict.bam",
        bed="baits_mapping/nlr_baits_regions.bed",
        bai="mappings/{sample}_strict.bam.bai",
        missing_genes="baits_mapping/missing_genes.txt"
    output:
        "coverage/{sample}_coverage.txt"
    conda:
        "envs/bedtools.yaml"
    log:
        "logs/bedtools/{sample}.log"
    resources:
        mem_mb=4000
    shell:
        "(coverageBed -d -a {input.bed} -b {input.bam} > {output}) 2> {log}"


rule per_gene_coverage:
    input:
        referenceGenes="resources/reference_headers_nlrs.txt",
        sample_coverage="coverage/{sample}_coverage.txt"
    output:
        gene_coverage="coverage/{sample}_geneCoverage.txt"
    log:
        "logs/coverage_gathering/{sample}_per_gene_coverage.log"
    resources:
        mem_mb=2000
    shell:
        """(cat {input.referenceGenes} | tail -n +2 | while read gene; do numPosWithCoverage=`grep -w "$gene" {input.sample_coverage} | awk '$6>0' | wc -l`; numPosTotal=`grep -w "$gene" {input.sample_coverage} | wc -l`; if [ $numPosTotal -eq 0 ]; then echo "ERROR: gene $gene has CDS region of length zero. Check your input data (e.g. gene spelling in FASTA and CDS BED file) and retry.\nAborting pipeline run." > {log}; exit; fi; pctCov=`awk "BEGIN {{print ($numPosWithCoverage/$numPosTotal)*100 }}"`; echo -e "\n# covered positions for sample {wildcards.sample} in gene $gene: $numPosWithCoverage\n# CDS positions for gene $gene: $numPosTotal\npctCov: $pctCov" >> {log}; echo -e "$gene\t$pctCov" >> {output.gene_coverage}; done) 2>> {log}"""


rule combine_gene_coverage:
    input:
        "coverage/{sample}_geneCoverage.txt"
    output:
        "coverage/{sample}_coverageValues.txt"
    log:
        "logs/coverage_gathering/{sample}_combine_gene_coverage.log"
    resources:
        mem_mb=2000
    run:
        shell("(echo {wildcards.sample} > {output}) 2> {log}")
        shell("(cat {input} | cut -f2 >> {output}) 2>> {log}")


rule combine_coverage_values:
    input:
        gene_names="resources/reference_headers_nlrs.txt",
        coverage=expand("coverage/{sample}_coverageValues.txt", sample=samples["sample"])
    output:
        "coverage/all_coverage_values.txt"
    log:
        "logs/coverage_gathering/combine_coverage_values.log"
    params:
        ulimit=config["ulimit"]
    resources:
        mem_mb=2000
    shell:
        """ulimit -n {params.ulimit}
        (paste {input.gene_names} {input.coverage} > {output}) 2> {log}"""


rule transpose_combined_coverage:
    input:
        "coverage/all_coverage_values.txt"
    output:
        "coverage/all_coverage_values_transposed.txt"
    log:
        "logs/coverage_gathering/transposing.log"
    resources:
        mem_mb=2000
    run:
        df = pd.read_table(input[0], header = None)
        df.T.to_csv(output[0], sep = "\t", header = False, index = False)
