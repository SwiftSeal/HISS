import pandas as pd
from snakemake.utils import validate

configfile: "config/config.yaml"

samples = pd.read_table(config["samples"], header = 0).set_index(["sample"], drop = False)
validate(samples, "sample_schema.yaml")

if samples.duplicated(subset = ["sample"]).any():
    sys.exit("Duplicate sample in samples file, check your inputs!")


def get_samples(widlcards):
    return samples["sample"][wildcards.sample]


def get_reads(wildcards):
    return samples["Reads"][wildcards.sample]


rule all:
    input:
        expand("NLR_coverage/{sample}_coverage_parsed.txt", sample = samples["sample"]),
        expand("assembly/{sample}_input_stats.txt", sample = samples["sample"]),
        "NLR_Annotator/NLR_summary.txt",
        "assembly/assembly_statistics.txt",
        expand("NLR_Annotator/{sample}_NLR_Annotator.fa", sample = samples["sample"])


rule trim_reads:
    input:
        get_reads
    params:
        fiveprime = config["fiveprime"],
        threeprime = config["threeprime"]
    output:
        fq = temp("trimmed_reads/{sample}.fq"),
        intermediate = temp("trimmed_reads/{sample}_intermediate.fq")
    threads:
        2
    conda:
        "envs/cutadapt.yaml"
    resources:
        mem_mb = 1000
    log:
        "logs/trim_reads/{sample}.log"
    shell:
        """
        cutadapt -j {threads} -g ^{params.fiveprime} -o {output.intermediate} {input} 2> {log}
        cutadapt -j {threads} -a {params.threeprime}$ -o {output.fq} {output.intermediate} 2>> {log}
        """


rule canu_assemble:
    input:
        "trimmed_reads/{sample}.fq"
    params:
        Prefix = lambda wildcards: samples["sample"][wildcards.sample],
        Genome_Size = config["Genome_Size"]
    output:
        assembly = "assembly/{sample}/{sample}.contigs.fasta",
        report = "assembly/{sample}/{sample}.report"
    threads:
        8
    conda:
        "envs/canu.yaml"
    resources:
        mem_mb = 36000,
        slurm_partition = "long"
    log:
        "logs/canu_assemble/{sample}.log"
    shell:
        """
        canu -d assembly/{wildcards.sample} -p {params.Prefix} -pacbio-hifi {input} useGrid=false genomeSize={params.Genome_Size} maxInputCoverage=20000 batMemory=32g 2> {log}
        """


rule summarise_assemblies:
    input:
        expand("assembly/{sample}/{sample}.contigs.fasta", sample = samples["sample"])
    output:
        "assembly/assembly_statistics.txt"
    conda:
        "envs/seqfu.yaml"
    resources:
        mem_mb = 1000
    log:
        "logs/summarise_assemblies/summarise_assemblies.log"
    shell:
        """
        seqfu stats -b {input} | sed 's/\.contigs//g' 1> {output} 2> {log}
        """


rule chop_sequences:
    input:
        "assembly/{sample}/{sample}.contigs.fasta"
    output:
        temp("NLR_Annotator/{sample}_chopped.fa")
    conda:
        "envs/meme.yaml"
    resources:
        mem_mb = 2000
    log:
        "logs/chop_sequences/{sample}.log"
    shell:
        """
        java -jar ../utils/ChopSequence.jar -i {input} -o {output} 2> {log}
        """


rule NLR_parser:
    input:
        "NLR_Annotator/{sample}_chopped.fa"
    output:
        temp("NLR_Annotator/{sample}_nlr_parser.xml")
    threads:
        2
    resources:
        mem_mb = 3000
    conda:
        "envs/meme.yaml"
    log:
        "logs/NLR_parser/{sample}.log"
    shell:
        """
        java -jar ../utils/NLR-Parser3.jar -t {threads} -y $(which mast) -x ../utils/meme.xml -i {input} -c {output} 2> {log}
        """


rule run_NLR_Annotator:
    input:
        parser_xml = "NLR_Annotator/{sample}_nlr_parser.xml",
        assembly = "assembly/{sample}/{sample}.contigs.fasta"
    output:
        text = "NLR_Annotator/{sample}_NLR_Annotator.txt",
        fasta = "NLR_Annotator/{sample}_NLR_Annotator.fa"
    params:
        flanking = config["flanking"]
    resources:
        mem_mb = 2000
    conda:
        "envs/meme.yaml"
    log:
        "logs/run_NLR_Annotator/{sample}.log"
    shell:
        """
        java -jar ../utils/NLR-Annotator.jar -i {input.parser_xml} -o {output.text} -f {input.assembly} {output.fasta} {params.flanking} 2> {log}
        """


rule summarise_NLRs:
    input:
        expand("NLR_Annotator/{sample}_NLR_Annotator.txt", sample = samples["sample"])
    output:
        "NLR_Annotator/NLR_summary.txt"
    resources:
        mem_mb = 1000
    log:
        "logs/summarise_NLRs/summarise_NLRs.log"
    run:
        import logging
        logging.basicConfig(filename={log}, encoding='utf-8', level=logging.DEBUG)
        with open(output[0], "w") as o:
            header_list = ["Sample", "NLR Contigs", "NLR Count", "Pseudogenous NLRs", "NLR Genes", "Complete NLRs", "Complete Pseudogenous NLRs"]
            header_string = "\t".join(header_list)
            o.write(header_string)
            o.write("\n")
            o.close()
        for file in input:
            filename_split = file.split('/')
            filename = filename_split[-1]
            sample = filename.replace('_NLR_Annotator.txt', '')
            lines = open(file).readlines()
            contig_set = set()
            count = 0
            pseudogenes = 0
            genes = 0
            complete = 0
            complete_pseudogenes = 0
            for line in lines:
                count += 1
                line = line.rstrip()
                split_line = line.split('\t')
                nlr_type = split_line[2]
                contig = split_line[0]
                contig_set.add(contig)
                if nlr_type == "complete (pseudogene)" or nlr_type == "partial (pseudogene)":
                    pseudogenes += 1
                if nlr_type == "complete" or nlr_type == "partial":
                    genes += 1
                if nlr_type == "complete":
                    complete += 1
                if nlr_type == "complete (pseudogene)":
                    complete_pseudogenes += 1
            contig_count = len(contig_set)
            with open(output[0], 'a') as o:
                list_to_write = [str(sample), str(contig_count), str(count), str(pseudogenes), str(genes), str(complete), str(complete_pseudogenes)]
                string_to_write = "\t".join(list_to_write)
                o.write(string_to_write)
                o.write("\n")
                o.close()


rule input_statistics:
    input:
        "assembly/{sample}/{sample}.report"
    output:
        "assembly/{sample}_input_stats.txt"
    resources:
        mem_mb = 1000
    log:
        "logs/input_statistics/{sample}.log"
    shell:
        """
        Reads=$(cat {input} | grep -m 1 'reads' | cut -f5 -d ' ') 2> {log}
        Bases=$(cat {input} | grep -m 1 'bases' | cut -f5 -d ' ') 2>> {log}
        printf "{wildcards.sample}\t$Reads\t$Bases" 1> {output} 2>> {log}
        """


rule convert_nlrs_to_bed:
    input:
        "NLR_Annotator/{sample}_NLR_Annotator.txt"
    output:
        temp("NLR_Annotator/{sample}_NLR_Annotator.bed")
    resources:
        mem_mb = 1000
    log:
        "logs/convert_nlrs_to_bed/{sample}.log"
    shell:
        """
        python3 workflow/scripts/NLR_Annotator_to_bed.py --input {input} --output {output} 2> {log}
        """


rule sort_nlr_bed:
    input:
        "NLR_Annotator/{sample}_NLR_Annotator.bed"
    output:
        "NLR_Annotator/{sample}_NLR_Annotator_sorted.bed"
    resources:
        mem_mb = 1000
    shell:
        """
        sort -k1,1V -k2,2n -k3,3n {input} > {output}
        """


rule map_hifi:
    input:
        reads = "trimmed_reads/{sample}.fq",
        assembly = "assembly/{sample}/{sample}.contigs.fasta"
    output:
        temp("mapping/{sample}_aligned.sam")
    threads:
        2
    conda:
        "envs/minimap2.yaml"
    resources:
        partition = "medium",
        mem_mb = 4000
    shell:
        """
        minimap2 -x map-hifi -t {threads} -a -o {output} {input.assembly} {input.reads}
        """


rule convert_sam_to_bam:
    input:
        "mapping/{sample}_aligned.sam"
    output:
        temp("mapping/{sample}_aligned.bam")
    conda:
        "envs/samtools.yaml"
    resources:
        mem_mb = 1000
    shell:
        """
        samtools view -F 256 {input} -b -o {output}
        """


rule sort_bam:
    input:
        "mapping/{sample}_aligned.bam"
    output:
        temp("mapping/{sample}_aligned_sorted.bam")
    conda:
        "envs/samtools.yaml"
    resources:
        mem_mb = 2000
    shell:
        """
        samtools sort {input} > {output}
        """


rule index_sorted_bam:
    input:
        "mapping/{sample}_aligned_sorted.bam"
    output:
        temp("mapping/{sample}_aligned_sorted.bam.bai")
    conda:
        "envs/samtools.yaml"
    resources:
        mem_mb = 1000
    shell:
        """
        samtools index {input} {output}
        """


rule calculate_coverage:
    input:
        bam = "mapping/{sample}_aligned_sorted.bam",
        index = "mapping/{sample}_aligned_sorted.bam.bai",
        bed = "NLR_Annotator/{sample}_NLR_Annotator_sorted.bed"
    output:
        temp("NLR_coverage/{sample}_NLR_coverage.txt")
    conda:
        "envs/samtools.yaml"
    resources:
        mem_mb = 1000
    shell:
        """
        samtools bedcov {input.bed} {input.bam} > {output}
        """


rule parse_coverage:
    input:
        "NLR_coverage/{sample}_NLR_coverage.txt"
    output:
        "NLR_coverage/{sample}_coverage_parsed.txt"
    resources:
        mem_mb = 1000
    shell:
        """
        python workflow/scripts/parse_coverage.py --input {input} --output {output}
        """
