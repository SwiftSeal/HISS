import pandas as pd

configfile: "config/config.yaml"

samples = pd.read_table(config["samples"], header=0).set_index(["sample"], drop=False)


def get_samples(widlcards):
    return samples["sample"][wildcards.sample]


def get_reads(wildcards):
    return samples["Reads"][wildcards.sample]


rule all:
    input:
        expand("NLR_coverage/{sample}_coverage_parsed.txt", sample=samples["sample"]),
        expand("assembly/{sample}_input_stats.txt", sample=samples["sample"]),
        "NLR_Annotator/NLR_summary.txt",
        "assembly/assembly_statistics.txt"


rule trim_reads:
    input:
        get_reads
    params:
        fiveprime=config["fiveprime"],
        threeprime=config["threeprime"]
    output:
        fq=temp("trimmed_reads/{sample}.fq"),
        intermediate=temp("trimmed_reads/{sample}_intermediate.fq")
    log:
        "logs/cutadapt/{sample}.log"
    threads:
        8
    conda:
        "envs/cutadapt.yaml"
    resources:
        mem_mb=4000
    shell:
        """(cutadapt -j {threads} -g ^{params.fiveprime} -o {output.intermediate} {input}) 2> {log}
        (cutadapt -j {threads} -a {params.threeprime}$ -o {output.fq} {output.intermediate}) 2>> {log}"""


rule canu_assemble:
    input:
        "trimmed_reads/{sample}.fq"
    params:
        Prefix=lambda wildcards: samples["sample"][wildcards.sample],
        Genome_Size=config["Genome_Size"]
    output:
        assembly="assembly/{sample}/{sample}.contigs.fasta",
        report="assembly/{sample}/{sample}.report"
    log:
        "logs/assembly/{sample}_assembly.log"
    threads:
        8
    conda:
        "envs/canu.yaml"
    resources:
        mem_mb=10000,
        partition="medium"
    shell:
        "(canu -d assembly/{wildcards.sample} -p {params.Prefix} -pacbio-hifi {input} useGrid=false genomeSize={params.Genome_Size} maxInputCoverage=10000) 2> {log}"


rule summarise_assemblies:
    input:
        expand("assembly/{sample}/{sample}.contigs.fasta", sample=samples["sample"])
    output:
        "assembly/assembly_statistics.txt"
    log:
        "logs/assembly/stats.log"
    conda:
        "envs/seqfu.yaml"
    resources:
        mem_mb=2000
    shell:
        "(seqfu stats -b {input} | sed 's/\.contigs//g' > {output}) 2> {log}"


rule NLR_Annotator:
    input:
        assembly="assembly/{sample}/{sample}.contigs.fasta"
    output:
        text="NLR_Annotator/{sample}_NLR_Annotator.txt",
        fasta="NLR_Annotator/{sample}_NLR_Annotator.fa"
    params:
        flanking=config["flanking"]
    log:
        "logs/NLR_Annotator/{sample}_annotator.log"
    resources:
        mem_mb=6000
    conda:
        "envs/openjdk.yaml"
    shell:
        "(java -jar ../utils/NLR-Annotator-v2.1.jar -x ../utils/mot.txt -y ../utils/store.txt -i {input.assembly} -o {output.text} -f {input.assembly} {output.fasta} {params.flanking}) 2> {log}"


rule summarise_NLRs:
    input:
        expand("NLR_Annotator/{sample}_NLR_Annotator.txt", sample=samples["sample"])
    output:
        "NLR_Annotator/NLR_summary.txt"
    resources:
        mem_mb=2000
    run:
        with open(output[0], "w") as o:
            header_list = ["Sample", "NLR Contigs", "NLR Count", "NBARC", "NBARC-LRR", "CC", "CC-NBARC", "CC-NBARC-LRR", "CC-LRR", "CC-TIR-NBARC-LRR", "TIR", "TIR-NBARC", "TIR-NBARC-LRR", "TIR-LRR", "TIR-CC-NBARC-LRR"]
            header_string = "\t".join(header_list)
            o.write(header_string)
            o.write("\n")
            o.close()
        for file in input:
            filename_split = file.split('/')
            filename = filename_split[-1]
            sample = filename.replace('_NLR_Annotator.txt', '')
            lines = open(file).readlines()
            contig_set = set()
            count = 0
            NBARC = 0
            NBARC_LRR = 0
            CC = 0
            CC_NBARC = 0
            CC_NBARC_LRR = 0
            CC_LRR = 0
            CC_TIR_NBARC_LRR = 0
            TIR = 0
            TIR_NBARC = 0
            TIR_NBARC_LRR = 0
            TIR_LRR = 0
            TIR_CC_NBARC_LRR = 0
            for line in lines:
                count += 1
                line = line.rstrip()
                split_line = line.split('\t')
                nlr_type = split_line[2]
                contig = split_line[0]
                contig_set.add(contig)
                if nlr_type == "NBARC":
                    NBARC += 1
                if nlr_type == "NBARC-LRR":
                    NBARC_LRR += 1
                if nlr_type == "CC":
                    CC += 1
                if nlr_type == "CC-NBARC":
                    CC_NBARC += 1
                if nlr_type == "CC-NBARC-LRR":
                    CC_NBARC_LRR += 1
                if nlr_type == "CC-LRR":
                    CC_LRR += 1
                if nlr_type == "CC-TIR-NBARC-LRR":
                    CC_TIR_NBARC_LRR += 1
                if nlr_type == "TIR":
                    TIR += 1
                if nlr_type == "TIR-NBARC":
                    TIR_NBARC += 1
                if nlr_type == "TIR-NBARC-LRR":
                    TIR_NBARC_LRR += 1
                if nlr_type == "TIR-LRR":
                    TIR_LRR += 1
                if nlr_type == "TIR-CC-NBARC-LRR":
                    TIR_CC_NBARC_LRR += 1
            contig_count = len(contig_set)
            with open(output[0], 'a') as o:
                list_to_write = [str(sample), str(contig_count), str(count), str(NBARC), str(NBARC_LRR), str(CC), str(CC_NBARC), str(CC_NBARC_LRR), str(CC_LRR), str(CC_TIR_NBARC_LRR), str(TIR), str(TIR_NBARC), str(TIR_NBARC_LRR), str(TIR_LRR), str(TIR_CC_NBARC_LRR)]
                string_to_write = "\t".join(list_to_write)
                o.write(string_to_write)
                o.write("\n")
                o.close()


rule input_statistics:
    input:
        "assembly/{sample}/{sample}.report"
    output:
        "assembly/{sample}_input_stats.txt"
    resources:
        mem_mb=2000
    shell:
        """Reads=$(cat {input} | grep -m 1 'reads' | cut -f5 -d ' ')
        Bases=$(cat {input} | grep -m 1 'bases' | cut -f5 -d ' ')
        printf "{wildcards.sample}\t$Reads\t$Bases" > {output}"""


rule convert_nlrs_to_bed:
    input:
        "NLR_Annotator/{sample}_NLR_Annotator.txt"
    output:
        temp("NLR_Annotator/{sample}_NLR_Annotator.bed")
    log:
        "logs/nlr_coverage/{sample}_convert_to_bed.log"
    resources:
        mem_mb=2000
    shell:
        "(python3 workflow/scripts/NLR_Annotator_to_bed.py --input {input} --output {output}) 2> {log}"


rule sort_nlr_bed:
    input:
        "NLR_Annotator/{sample}_NLR_Annotator.bed"
    output:
        "NLR_Annotator/{sample}_NLR_Annotator_sorted.bed"
    log:
        "logs/nlr_coverage/{sample}_sortbed.log"
    resources:
        mem_mb=2000
    shell:
        "(sort -k1,1V -k2,2n -k3,3n {input} > {output}) 2> {log}"


rule map_hifi:
    input:
        reads="trimmed_reads/{sample}.fq",
        assembly="assembly/{sample}/{sample}.contigs.fasta"
    output:
        temp("mapping/{sample}_aligned.sam")
    log:
        "logs/nlr_coverage/{sample}_map_hifi.log"
    conda:
        "envs/minimap2.yaml"
    resources:
        partition="medium",
        mem_mb=4000
    shell:
        "(minimap2 -x map-hifi -a -o {output} {input.assembly} {input.reads}) 2> {log}"


rule convert_sam_to_bam:
    input:
        "mapping/{sample}_aligned.sam"
    output:
        temp("mapping/{sample}_aligned.bam")
    log:
        "logs/nlr_coverage/{sample}_convert_to_bam.log"
    conda:
        "envs/samtools.yaml"
    resources:
        mem_mb=4000
    shell:
        "(samtools view -F 256 {input} -b -o {output}) 2> {log}"


rule sort_bam:
    input:
        "mapping/{sample}_aligned.bam"
    output:
        "mapping/{sample}_aligned_sorted.bam"
    log:
        "logs/nlr_coverage/{sample}_sort_bam.log"
    conda:
        "envs/samtools.yaml"
    resources:
        mem_mb=4000
    shell:
        "(samtools sort {input} > {output}) 2> {log}"


rule index_sorted_bam:
    input:
        "mapping/{sample}_aligned_sorted.bam"
    output:
        "mapping/{sample}_aligned_sorted.bam.bai"
    log:
        "logs/nlr_coverage/{sample}_index_sorted_bam.log"
    conda:
        "envs/samtools.yaml"
    resources:
        mem_mb=4000
    shell:
        "(samtools index {input} {output}) > {log}"


rule calculate_coverage:
    input:
        bam="mapping/{sample}_aligned_sorted.bam",
        index="mapping/{sample}_aligned_sorted.bam.bai",
        bed="NLR_Annotator/{sample}_NLR_Annotator_sorted.bed"
    output:
        "NLR_coverage/{sample}_NLR_coverage.txt"
    log:
        "logs/nlr_coverage/{sample}_calculate_coverage.log"
    conda:
        "envs/samtools.yaml"
    resources:
        mem_mb=4000
    shell:
        "(samtools bedcov {input.bed} {input.bam} > {output}) 2> {log}"


rule parse_coverage:
    input:
        "NLR_coverage/{sample}_NLR_coverage.txt"
    output:
        "NLR_coverage/{sample}_coverage_parsed.txt"
    log:
        "logs/nlr_coverage/{sample}_parse_coverage.log"
    resources:
        mem_mb=2000
    shell:
        "(python workflow/scripts/parse_coverage.py --input {input} --output {output}) 2> {log}"
